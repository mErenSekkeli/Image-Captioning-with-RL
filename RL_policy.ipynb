{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': '03577_0',\n",
       "  'sentence1': 'a big blue building has been built in the middle of the scene ',\n",
       "  'sentence2': 'the bareland has been replaced by a blue building and a big blue building has been built ',\n",
       "  'sentence3': 'two big blue buildings have been replaced by a blue building '},\n",
       " {'filename': '03577_1',\n",
       "  'sentence1': 'a small blue building has been constructed on the bareland at the corner of the scene ',\n",
       "  'sentence2': 'many trees have appeared in many parts of the scene ',\n",
       "  'sentence3': 'many green trees have been constructed on the bareland and the grassland '},\n",
       " {'filename': '03577_2',\n",
       "  'sentence1': 'a big building has been constructed in many parts of the scene ',\n",
       "  'sentence2': 'a blue building has been constructed in the green area ',\n",
       "  'sentence3': 'a big building has been constructed in the green area '},\n",
       " {'filename': '03577_3',\n",
       "  'sentence1': 'two blue buildings have been built in many parts of the scene ',\n",
       "  'sentence2': 'two blue buildings have been built at the corner of the scene ',\n",
       "  'sentence3': 'the trees have grown up in many parts of the scene '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_data import load_json_data\n",
    "\n",
    "# Load data\n",
    "data = load_json_data(\"data/result.json\", 1000, 1520)\n",
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path: RL_policy.ipynb\n",
    "from load_data import load_extracted_features\n",
    "features = load_extracted_features(\"02243\")\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_extracted_features\n",
    "training_data = []\n",
    "\n",
    "for item in data:\n",
    "    item = item[0]\n",
    "    if \"slider1\" not in item:\n",
    "        continue\n",
    "    file_name = item['filename'].split(\"_\")[0]\n",
    "    print(file_name)\n",
    "    features = load_extracted_features(file_name)\n",
    "    training_data.append({\n",
    "        \"features\": features,\n",
    "        \"sentences\": [item[\"sentence1\"], item[\"sentence2\"], item[\"sentence3\"]],\n",
    "        \"slider_values\": [item[\"slider1\"], item[\"slider2\"], item[\"slider3\"]]\n",
    "    })\n",
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning Class\n",
    "* Define Q-Learning Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearningModel:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.1, discount_rate=0.9, exploration_rate=0.5):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_rate * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Python\\bitirme\\final_project\\RL_policy.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/bitirme/final_project/RL_policy.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m\"\u001b[39m, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/bitirme/final_project/RL_policy.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage reward: \u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m.\u001b[39mevaluate_performance(training_data))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Python/bitirme/final_project/RL_policy.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m state_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(training_data[\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(training_data[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/bitirme/final_project/RL_policy.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m action_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(training_data[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/bitirme/final_project/RL_policy.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_performance(model, training_data):\n",
    "        total_reward = 0\n",
    "        for i in range(model.state_size):\n",
    "            state = i\n",
    "            best_action = np.argmax(model.q_table[state])\n",
    "            total_reward += training_data[state]['slider_values'][best_action]\n",
    "        average_reward = total_reward / model.state_size\n",
    "        return average_reward\n",
    "\n",
    "def visualize_performance(performance_history):\n",
    "    plt.plot(performance_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average reward\")\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, training_data, epochs):\n",
    "    performance_history = []\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(epochs), desc=\"Training\"):\n",
    "        for i, data in enumerate(training_data):\n",
    "            state = i % model.state_size\n",
    "            action = model.choose_action(state)\n",
    "            reward = data['slider_values'][action]\n",
    "            next_state = (i + 1) % model.state_size if i + 1 < len(training_data) else state\n",
    "            model.update_q_table(state, action, reward, next_state)\n",
    "            \n",
    "        average_reward = evaluate_performance(model, training_data)\n",
    "        print(\"Epoch: \", epoch)\n",
    "        print(\"Average reward: \", average_reward(training_data))\n",
    "        performance_history.append(average_reward)\n",
    "\n",
    "    return performance_history\n",
    "\n",
    "\n",
    "state_size = len(training_data['features'][0]) * len(training_data[0]['features'])\n",
    "action_size = len(training_data[0]['sentences'])\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 0.5\n",
    "epochs = 100\n",
    "\n",
    "model = QLearningModel(state_size, action_size, learning_rate, discount_factor, exploration_rate)\n",
    "performance_history = train_model(model, training_data, epochs, learning_rate, discount_factor)\n",
    "visualize_performance(performance_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "requirements",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
